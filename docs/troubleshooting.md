# æ•…éšœæ’é™¤æŒ‡å—

AI é‡åŒ–ç³»ç»Ÿå¸¸è§é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ

## æ•…éšœæ’é™¤æ€»ä½“æµç¨‹

### 1. é—®é¢˜åˆ†ç±»

| ç±»å‹          | ç—‡çŠ¶                   | ä¼˜å…ˆçº§ |
| ------------- | ---------------------- | ------ |
| ğŸ”´ **ç³»ç»Ÿçº§** | æœåŠ¡æ— æ³•å¯åŠ¨ã€ç³»ç»Ÿå´©æºƒ | ç´§æ€¥   |
| ğŸŸ¡ **åŠŸèƒ½çº§** | ç‰¹å®šåŠŸèƒ½å¼‚å¸¸ã€æ•°æ®é”™è¯¯ | é«˜     |
| ğŸŸ¢ **æ€§èƒ½çº§** | å“åº”æ…¢ã€èµ„æºå ç”¨é«˜     | ä¸­     |
| ğŸ”µ **é…ç½®çº§** | ç¯å¢ƒé…ç½®ã€å‚æ•°è°ƒæ•´     | ä½     |

### 2. è¯Šæ–­æ­¥éª¤

```mermaid
graph TD
    A[é—®é¢˜æŠ¥å‘Š] --> B[æ”¶é›†æ—¥å¿—ä¿¡æ¯]
    B --> C[æ£€æŸ¥ç³»ç»ŸçŠ¶æ€]
    C --> D[å®šä½é—®é¢˜èŒƒå›´]
    D --> E{é—®é¢˜ç±»å‹}
    E -->|ç³»ç»Ÿçº§| F[ç³»ç»Ÿæ•…éšœå¤„ç†]
    E -->|åŠŸèƒ½çº§| G[åŠŸèƒ½æ•…éšœå¤„ç†]
    E -->|æ€§èƒ½çº§| H[æ€§èƒ½é—®é¢˜åˆ†æ]
    E -->|é…ç½®çº§| I[é…ç½®é—®é¢˜ä¿®å¤]
    F --> J[éªŒè¯ä¿®å¤ç»“æœ]
    G --> J
    H --> J
    I --> J
    J --> K[æ–‡æ¡£è®°å½•]
```

## ç³»ç»Ÿçº§æ•…éšœå¤„ç†

### 1. æœåŠ¡å¯åŠ¨å¤±è´¥

#### ğŸ”´ ç—‡çŠ¶

- Docker å®¹å™¨å¯åŠ¨å¤±è´¥
- åº”ç”¨è¿›ç¨‹æ— æ³•å¯åŠ¨
- ç«¯å£ç»‘å®šé”™è¯¯

#### ğŸ” è¯Šæ–­å‘½ä»¤

```bash
# æ£€æŸ¥å®¹å™¨çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker-compose logs app

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tulpn | grep :8000
lsof -i :8000

# æ£€æŸ¥ç³»ç»Ÿèµ„æº
df -h
free -h
top
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**ç«¯å£å ç”¨é—®é¢˜**:

```bash
# æŸ¥æ‰¾å ç”¨è¿›ç¨‹
sudo netstat -tulpn | grep :8000

# ç»ˆæ­¢è¿›ç¨‹
sudo kill -9 <PID>

# é‡å¯æœåŠ¡
docker-compose restart app
```

**èµ„æºä¸è¶³é—®é¢˜**:

```bash
# æ¸…ç†Dockerèµ„æº
docker system prune -af
docker volume prune -f

# æ¸…ç†æ—¥å¿—æ–‡ä»¶
sudo truncate -s 0 /var/log/syslog
find logs/ -name "*.log" -exec truncate -s 0 {} \;
```

**é…ç½®æ–‡ä»¶é”™è¯¯**:

```bash
# æ£€æŸ¥é…ç½®æ–‡ä»¶è¯­æ³•
python -c "from backend.app.core.config import get_settings; print(get_settings())"

# é‡ç½®é…ç½®æ–‡ä»¶
cp env.template .env
# é‡æ–°ç¼–è¾‘é…ç½®
```

### 2. æ•°æ®åº“è¿æ¥å¤±è´¥

#### ğŸ”´ ç—‡çŠ¶

- åº”ç”¨æ— æ³•è¿æ¥æ•°æ®åº“
- æ•°æ®åº“æŸ¥è¯¢è¶…æ—¶
- è¿æ¥æ± è€—å°½

#### ğŸ” è¯Šæ–­å‘½ä»¤

```bash
# æ£€æŸ¥PostgreSQLçŠ¶æ€
docker-compose exec postgres pg_isready -U ai_quant_user

# æµ‹è¯•æ•°æ®åº“è¿æ¥
psql -h localhost -p 5432 -U ai_quant_user -d ai_quant_db

# æ£€æŸ¥è¿æ¥æ•°
psql -c "SELECT count(*) as connections FROM pg_stat_activity;"
psql -c "SELECT * FROM pg_stat_activity WHERE state = 'active';"
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**è¿æ¥è¶…æ—¶é—®é¢˜**:

```bash
# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
ping postgres_container_ip

# è°ƒæ•´è¿æ¥æ± é…ç½®
# åœ¨.envæ–‡ä»¶ä¸­å¢åŠ :
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=0
DB_POOL_TIMEOUT=30
```

**è¿æ¥æ•°è¿‡å¤š**:

```sql
-- æŸ¥çœ‹è¿æ¥è¯¦æƒ…
SELECT pid, usename, application_name, client_addr, state, query_start
FROM pg_stat_activity
WHERE state = 'active';

-- ç»ˆæ­¢ç©ºé—²è¿æ¥
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state = 'idle' AND query_start < now() - interval '30 minutes';
```

**æƒé™é—®é¢˜**:

```sql
-- æ£€æŸ¥ç”¨æˆ·æƒé™
\du ai_quant_user

-- é‡æ–°æˆæƒ
GRANT ALL PRIVILEGES ON DATABASE ai_quant_db TO ai_quant_user;
GRANT USAGE ON SCHEMA public TO ai_quant_user;
GRANT ALL ON ALL TABLES IN SCHEMA public TO ai_quant_user;
```

### 3. Redis è¿æ¥é—®é¢˜

#### ğŸ”´ ç—‡çŠ¶

- ç¼“å­˜æœåŠ¡ä¸å¯ç”¨
- Redis å†…å­˜æº¢å‡º
- æ•°æ®æŒä¹…åŒ–å¤±è´¥

#### ğŸ” è¯Šæ–­å‘½ä»¤

```bash
# æ£€æŸ¥RedisçŠ¶æ€
docker-compose exec redis redis-cli ping

# æŸ¥çœ‹Redisä¿¡æ¯
docker-compose exec redis redis-cli info

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
docker-compose exec redis redis-cli info memory
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**å†…å­˜æº¢å‡ºé—®é¢˜**:

```bash
# æ¸…ç†è¿‡æœŸé”®
docker-compose exec redis redis-cli --scan --pattern "*" | xargs docker-compose exec redis redis-cli del

# è°ƒæ•´å†…å­˜é…ç½®
# åœ¨redis.confä¸­è®¾ç½®:
maxmemory 1gb
maxmemory-policy allkeys-lru
```

**æŒä¹…åŒ–å¤±è´¥**:

```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ‰‹åŠ¨è§¦å‘ä¿å­˜
docker-compose exec redis redis-cli bgsave

# æ£€æŸ¥AOFæ–‡ä»¶
docker-compose exec redis redis-cli info persistence
```

## åŠŸèƒ½çº§æ•…éšœå¤„ç†

### 1. æ•°æ®è·å–å¼‚å¸¸

#### ğŸŸ¡ ç—‡çŠ¶

- AKShare API è°ƒç”¨å¤±è´¥
- æ•°æ®è¿”å›ä¸ºç©º
- æ•°æ®æ ¼å¼é”™è¯¯

#### ğŸ” è¯Šæ–­æ­¥éª¤

```bash
# æµ‹è¯•AKShareè¿æ¥
python3 -c "
import akshare as ak
try:
    df = ak.stock_info_sh_name_code()
    print(f'è·å–åˆ° {len(df)} æ¡æ•°æ®')
except Exception as e:
    print(f'é”™è¯¯: {e}')
"

# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://akshare.akfamily.xyz/

# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
docker-compose logs app | grep -i "akshare\|error"
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**API é™æµé—®é¢˜**:

```python
# è°ƒæ•´é‡è¯•é…ç½®
RETRY_CONFIG = {
    "max_retries": 5,
    "delay": 2.0,
    "backoff": 2.0,
    "max_delay": 60.0
}
```

**ç½‘ç»œé—®é¢˜**:

```bash
# é…ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
export http_proxy=http://proxy.company.com:8080
export https_proxy=http://proxy.company.com:8080

# æˆ–åœ¨docker-compose.ymlä¸­æ·»åŠ ï¼š
environment:
  - http_proxy=http://proxy.company.com:8080
  - https_proxy=http://proxy.company.com:8080
```

**æ•°æ®æ ¼å¼é—®é¢˜**:

```python
# æ·»åŠ æ•°æ®éªŒè¯
def validate_stock_data(df):
    required_columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡']
    missing_cols = [col for col in required_columns if col not in df.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
    return True
```

### 2. æ•°æ®å­˜å‚¨å¼‚å¸¸

#### ğŸŸ¡ ç—‡çŠ¶

- Parquet æ–‡ä»¶æŸå
- æ•°æ®å†™å…¥å¤±è´¥
- æŸ¥è¯¢ç»“æœä¸ä¸€è‡´

#### ğŸ” è¯Šæ–­æ­¥éª¤

```bash
# æ£€æŸ¥å­˜å‚¨ç›®å½•æƒé™
ls -la data/parquet/

# æµ‹è¯•Parquetæ–‡ä»¶å®Œæ•´æ€§
python3 -c "
import pandas as pd
import os
for root, dirs, files in os.walk('data/parquet'):
    for file in files:
        if file.endswith('.parquet'):
            try:
                df = pd.read_parquet(os.path.join(root, file))
                print(f'âœ“ {file}: {len(df)} rows')
            except Exception as e:
                print(f'âœ— {file}: {e}')
"

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h data/
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**æ–‡ä»¶æƒé™é—®é¢˜**:

```bash
# ä¿®å¤æƒé™
sudo chown -R $(whoami):$(whoami) data/
chmod -R 755 data/
```

**ç£ç›˜ç©ºé—´ä¸è¶³**:

```bash
# æ¸…ç†æ—§æ•°æ®
find data/parquet -name "*.parquet" -mtime +90 -delete

# å¯ç”¨å‹ç¼©
# åœ¨å­˜å‚¨é…ç½®ä¸­å¯ç”¨å‹ç¼©ï¼š
PARQUET_COMPRESSION = "snappy"
```

**æ–‡ä»¶æŸåä¿®å¤**:

```python
# æ•°æ®æ¢å¤è„šæœ¬
def repair_parquet_file(file_path):
    try:
        df = pd.read_parquet(file_path)
        # é‡æ–°ä¿å­˜ä¿®å¤
        df.to_parquet(file_path + ".repaired", compression='snappy')
        os.rename(file_path + ".repaired", file_path)
        print(f"ä¿®å¤æˆåŠŸ: {file_path}")
    except Exception as e:
        print(f"æ— æ³•ä¿®å¤: {file_path}, é”™è¯¯: {e}")
```

### 3. API å“åº”å¼‚å¸¸

#### ğŸŸ¡ ç—‡çŠ¶

- è¯·æ±‚è¶…æ—¶
- è¿”å› 500 é”™è¯¯
- æ•°æ®æ ¼å¼ä¸æ­£ç¡®

#### ğŸ” è¯Šæ–­æ­¥éª¤

```bash
# æµ‹è¯•APIç«¯ç‚¹
curl -v http://localhost:8000/api/v1/health/ping

# æ£€æŸ¥APIæ—¥å¿—
docker-compose logs app | grep -E "(ERROR|500|timeout)"

# æ€§èƒ½æµ‹è¯•
ab -n 100 -c 10 http://localhost:8000/api/v1/health/ping
```

#### âš¡ è§£å†³æ–¹æ¡ˆ

**è¶…æ—¶é—®é¢˜**:

```python
# è°ƒæ•´è¶…æ—¶é…ç½®
TIMEOUT_CONFIG = {
    "request_timeout": 60,
    "database_timeout": 30,
    "redis_timeout": 5
}
```

**å†…å­˜æ³„æ¼**:

```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
docker stats

# é‡å¯åº”ç”¨æ¸…ç†å†…å­˜
docker-compose restart app
```

## æ€§èƒ½é—®é¢˜åˆ†æ

### 1. å“åº”æ—¶é—´æ…¢

#### ğŸŸ¢ ç—‡çŠ¶

- API å“åº”æ—¶é—´ > 200ms
- æ•°æ®æŸ¥è¯¢ç¼“æ…¢
- é¡µé¢åŠ è½½æ…¢

#### ğŸ” æ€§èƒ½åˆ†æ

```bash
# APIå“åº”æ—¶é—´æµ‹è¯•
curl -w "@curl-format.txt" -o /dev/null -s http://localhost:8000/api/v1/data/stocks/000001

# curl-format.txtå†…å®¹:
#     time_namelookup:  %{time_namelookup}\n
#        time_connect:  %{time_connect}\n
#     time_appconnect:  %{time_appconnect}\n
#    time_pretransfer:  %{time_pretransfer}\n
#       time_redirect:  %{time_redirect}\n
#  time_starttransfer:  %{time_starttransfer}\n
#                     ----------\n
#          time_total:  %{time_total}\n

# æ•°æ®åº“æŸ¥è¯¢åˆ†æ
psql -c "SELECT query, mean_time, calls FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 10;"
```

#### âš¡ ä¼˜åŒ–æ–¹æ¡ˆ

**æ•°æ®åº“ä¼˜åŒ–**:

```sql
-- åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_stock_symbol_date ON stock_data(symbol, date);

-- åˆ†æè¡¨ç»Ÿè®¡
ANALYZE stock_data;

-- æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’
EXPLAIN ANALYZE SELECT * FROM stock_data WHERE symbol = '000001';
```

**ç¼“å­˜ä¼˜åŒ–**:

```python
# å¢åŠ ç¼“å­˜å±‚
@cache.memoize(timeout=300)
def get_stock_data(symbol, start_date, end_date):
    return fetch_data_from_storage(symbol, start_date, end_date)
```

**ä»£ç ä¼˜åŒ–**:

```python
# å¼‚æ­¥å¤„ç†
async def batch_process_stocks(symbols):
    tasks = [process_single_stock(symbol) for symbol in symbols]
    return await asyncio.gather(*tasks)
```

### 2. å†…å­˜ä½¿ç”¨è¿‡é«˜

#### ğŸŸ¢ ç—‡çŠ¶

- å†…å­˜ä½¿ç”¨ç‡ > 80%
- OOM é”™è¯¯
- å®¹å™¨é‡å¯é¢‘ç¹

#### ğŸ” å†…å­˜åˆ†æ

```bash
# æ£€æŸ¥å®¹å™¨å†…å­˜ä½¿ç”¨
docker stats --no-stream

# Pythonå†…å­˜åˆ†æ
python3 -c "
import psutil
import os
process = psutil.Process(os.getpid())
print(f'å†…å­˜ä½¿ç”¨: {process.memory_info().rss / 1024 / 1024:.1f} MB')
"

# æ£€æŸ¥å¤§å¯¹è±¡
# å¯ä»¥ä½¿ç”¨memory_profiler
pip install memory-profiler
python -m memory_profiler your_script.py
```

#### âš¡ ä¼˜åŒ–æ–¹æ¡ˆ

**æ•°æ®åˆ†æ‰¹å¤„ç†**:

```python
def process_large_dataset(data, batch_size=1000):
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        yield process_batch(batch)
```

**å†…å­˜é™åˆ¶é…ç½®**:

```yaml
# docker-compose.yml
services:
  app:
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
```

### 3. CPU ä½¿ç”¨ç‡é«˜

#### ğŸŸ¢ ç—‡çŠ¶

- CPU ä½¿ç”¨ç‡ > 90%
- ç³»ç»Ÿå“åº”æ…¢
- é£æ‰‡é«˜é€Ÿè¿è½¬

#### ğŸ” CPU åˆ†æ

```bash
# å®æ—¶ç›‘æ§
htop

# CPUä½¿ç”¨è¯¦æƒ…
pidstat -u 1

# Pythonæ€§èƒ½åˆ†æ
python -m cProfile -o profile.stats your_script.py
python -c "
import pstats
p = pstats.Stats('profile.stats')
p.sort_stats('cumulative').print_stats(10)
"
```

#### âš¡ ä¼˜åŒ–æ–¹æ¡ˆ

**å¹¶å‘æ§åˆ¶**:

```python
# é™åˆ¶å¹¶å‘æ•°
semaphore = asyncio.Semaphore(5)

async def rate_limited_task():
    async with semaphore:
        await actual_task()
```

**ç®—æ³•ä¼˜åŒ–**:

```python
# ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„
from collections import deque
import bisect

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
import re
pattern = re.compile(r'your_pattern')
```

## é…ç½®é—®é¢˜ä¿®å¤

### 1. ç¯å¢ƒå˜é‡é…ç½®

#### ğŸ”µ å¸¸è§é…ç½®é”™è¯¯

```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
printenv | grep -E "(DB_|REDIS_|SECRET_)"

# éªŒè¯é…ç½®
python3 -c "
from backend.app.core.config import get_settings
settings = get_settings()
print('æ•°æ®åº“:', settings.DB_HOST)
print('Redis:', settings.REDIS_HOST)
print('å¯†é’¥é•¿åº¦:', len(settings.SECRET_KEY))
"
```

#### âš¡ é…ç½®ä¿®å¤

**æ•°æ®åº“é…ç½®**:

```bash
# .envæ–‡ä»¶æ£€æŸ¥
cat .env | grep -v "^#" | grep -v "^$"

# å¿…è¦é…ç½®é¡¹
DB_HOST=postgres
DB_PORT=5432
DB_USERNAME=ai_quant_user
DB_PASSWORD=your_secure_password
DB_NAME=ai_quant_db
```

**å®‰å…¨é…ç½®**:

```bash
# ç”Ÿæˆå®‰å…¨å¯†é’¥
python3 -c "
import secrets
print('SECRET_KEY=' + secrets.token_urlsafe(32))
"
```

### 2. æ—¥å¿—é…ç½®

#### ğŸ”µ æ—¥å¿—é—®é¢˜è¯Šæ–­

```bash
# æ£€æŸ¥æ—¥å¿—ç›®å½•
ls -la logs/

# æ£€æŸ¥æ—¥å¿—æƒé™
ls -la logs/*.log

# æ£€æŸ¥æ—¥å¿—è½®è½¬é…ç½®
cat /etc/logrotate.d/ai-quant
```

#### âš¡ æ—¥å¿—é…ç½®ä¿®å¤

**æ—¥å¿—æƒé™ä¿®å¤**:

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs
chmod 755 logs
touch logs/app.log
chmod 644 logs/app.log
```

**æ—¥å¿—çº§åˆ«è°ƒæ•´**:

```python
# ä¸´æ—¶è°ƒæ•´æ—¥å¿—çº§åˆ«
import logging
logging.getLogger().setLevel(logging.DEBUG)

# é…ç½®æ–‡ä»¶è°ƒæ•´
LOG_LEVEL=DEBUG  # è°ƒè¯•æ—¶ä½¿ç”¨
LOG_LEVEL=INFO   # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨
```

## ç›‘æ§å’Œé¢„é˜²

### 1. å¥åº·æ£€æŸ¥è„šæœ¬

```bash
#!/bin/bash
# health_check.sh

echo "=== AIé‡åŒ–ç³»ç»Ÿå¥åº·æ£€æŸ¥ $(date) ==="

# APIå¥åº·æ£€æŸ¥
echo "1. APIå¥åº·æ£€æŸ¥..."
if curl -f -s http://localhost:8000/api/v1/health/ping > /dev/null; then
    echo "   âœ“ APIæœåŠ¡æ­£å¸¸"
else
    echo "   âœ— APIæœåŠ¡å¼‚å¸¸"
fi

# æ•°æ®åº“å¥åº·æ£€æŸ¥
echo "2. æ•°æ®åº“å¥åº·æ£€æŸ¥..."
if docker-compose exec -T postgres pg_isready -q; then
    echo "   âœ“ æ•°æ®åº“æ­£å¸¸"
else
    echo "   âœ— æ•°æ®åº“å¼‚å¸¸"
fi

# Rediså¥åº·æ£€æŸ¥
echo "3. Rediså¥åº·æ£€æŸ¥..."
if docker-compose exec -T redis redis-cli ping | grep -q PONG; then
    echo "   âœ“ Redisæ­£å¸¸"
else
    echo "   âœ— Rediså¼‚å¸¸"
fi

# ç£ç›˜ç©ºé—´æ£€æŸ¥
echo "4. ç£ç›˜ç©ºé—´æ£€æŸ¥..."
DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
if [ $DISK_USAGE -lt 90 ]; then
    echo "   âœ“ ç£ç›˜ç©ºé—´å……è¶³ ($DISK_USAGE%)"
else
    echo "   âš  ç£ç›˜ç©ºé—´ä¸è¶³ ($DISK_USAGE%)"
fi

# å†…å­˜ä½¿ç”¨æ£€æŸ¥
echo "5. å†…å­˜ä½¿ç”¨æ£€æŸ¥..."
MEMORY_USAGE=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
echo "   å†…å­˜ä½¿ç”¨ç‡: $MEMORY_USAGE%"

echo "=== å¥åº·æ£€æŸ¥å®Œæˆ ==="
```

### 2. è‡ªåŠ¨åŒ–ç›‘æ§

```bash
#!/bin/bash
# monitor.sh

# è®¾ç½®å‘Šè­¦é˜ˆå€¼
CPU_THRESHOLD=80
MEMORY_THRESHOLD=85
DISK_THRESHOLD=90

# æ£€æŸ¥CPUä½¿ç”¨ç‡
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
if (( $(echo "$CPU_USAGE > $CPU_THRESHOLD" | bc -l) )); then
    echo "å‘Šè­¦: CPUä½¿ç”¨ç‡è¿‡é«˜ ($CPU_USAGE%)"
    # å‘é€å‘Šè­¦é€šçŸ¥
    curl -X POST "https://api.telegram.org/bot$BOT_TOKEN/sendMessage" \
         -d "chat_id=$CHAT_ID&text=AIé‡åŒ–ç³»ç»ŸCPUå‘Šè­¦: $CPU_USAGE%"
fi

# æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
MEMORY_USAGE=$(free | grep Mem | awk '{printf "%.1f", $3/$2 * 100.0}')
if (( $(echo "$MEMORY_USAGE > $MEMORY_THRESHOLD" | bc -l) )); then
    echo "å‘Šè­¦: å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ($MEMORY_USAGE%)"
fi

# å®šæœŸè¿è¡Œç›‘æ§
# æ·»åŠ åˆ°crontab: */5 * * * * /path/to/monitor.sh
```

### 3. é¢„é˜²æ€§ç»´æŠ¤

```bash
#!/bin/bash
# maintenance.sh

echo "å¼€å§‹é¢„é˜²æ€§ç»´æŠ¤..."

# 1. æ¸…ç†æ—¥å¿—
find logs/ -name "*.log" -size +100M -exec truncate -s 50M {} \;

# 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
find /tmp -name "tmp*" -mtime +7 -delete

# 3. æ•°æ®åº“ç»´æŠ¤
docker-compose exec -T postgres psql -U ai_quant_user -d ai_quant_db -c "VACUUM ANALYZE;"

# 4. Rediså†…å­˜ä¼˜åŒ–
docker-compose exec -T redis redis-cli MEMORY PURGE

# 5. Dockerç³»ç»Ÿæ¸…ç†
docker system prune -f --volumes

echo "é¢„é˜²æ€§ç»´æŠ¤å®Œæˆ"
```

## ç´§æ€¥æ•…éšœå¤„ç†

### åº”æ€¥å“åº”æµç¨‹

1. **ç«‹å³å“åº”** (5 åˆ†é’Ÿå†…)

   - ç¡®è®¤æ•…éšœå½±å“èŒƒå›´
   - å¯åŠ¨åº”æ€¥é¢„æ¡ˆ
   - é€šçŸ¥ç›¸å…³äººå‘˜

2. **å¿«é€Ÿæ¢å¤** (15 åˆ†é’Ÿå†…)

   - å°è¯•æœåŠ¡é‡å¯
   - åˆ‡æ¢å¤‡ç”¨æ–¹æ¡ˆ
   - æ•°æ®å›æ»š(å¦‚éœ€è¦)

3. **æ ¹å› åˆ†æ** (1 å°æ—¶å†…)

   - æ”¶é›†æ•…éšœè¯æ®
   - åˆ†ææ ¹æœ¬åŸå› 
   - åˆ¶å®šä¿®å¤æ–¹æ¡ˆ

4. **ä¿®å¤éªŒè¯** (4 å°æ—¶å†…)
   - å®æ–½ä¿®å¤æ–¹æ¡ˆ
   - å…¨é¢åŠŸèƒ½æµ‹è¯•
   - æ€§èƒ½éªŒè¯

### åº”æ€¥è”ç³»æ–¹å¼

```
è¿ç»´å›¢é˜Ÿ: ops@company.com
å¼€å‘å›¢é˜Ÿ: dev@company.com
24å°æ—¶çƒ­çº¿: +86-xxx-xxxx-xxxx

é’‰é’‰ç¾¤: AIé‡åŒ–ç³»ç»Ÿè¿ç»´
å¾®ä¿¡ç¾¤: é‡åŒ–ç³»ç»ŸæŠ€æœ¯æ”¯æŒ
```

---

**æ•…éšœæ’é™¤æŒ‡å—ç‰ˆæœ¬**: v1.0.0  
**æ›´æ–°æ—¶é—´**: 2024 å¹´ 12 æœˆ 09 æ—¥  
**ç»´æŠ¤å›¢é˜Ÿ**: æŠ€æœ¯è¿ç»´éƒ¨
