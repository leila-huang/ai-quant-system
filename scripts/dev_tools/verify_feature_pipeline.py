#!/usr/bin/env python3
"""
ç‰¹å¾å·¥ç¨‹æ•°æ®æµæ°´çº¿éªŒè¯è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import tempfile
import shutil
from datetime import date, timedelta
import time
import pandas as pd
import numpy as np

from backend.src.models.basic_models import StockData, StockDailyBar
from backend.src.engine.features.pipeline import (
    FeaturePipeline, PipelineConfig, ScalerType, FeatureSelectionMethod,
    create_default_pipeline
)
from backend.src.engine.features.feature_store import FeatureStore, create_default_feature_store
from backend.src.data.akshare_adapter import AKShareAdapter


def create_test_stock_data(symbol: str, days: int = 100) -> StockData:
    """åˆ›å»ºæµ‹è¯•è‚¡ç¥¨æ•°æ®"""
    base_date = date(2024, 1, 1)
    bars = []
    
    np.random.seed(hash(symbol) % 2147483647)  # åŸºäºè‚¡ç¥¨ä»£ç çš„å¯é‡å¤éšæœºç§å­
    
    base_price = 10.0 + np.random.random() * 20.0  # 10-30çš„åŸºç¡€ä»·æ ¼
    
    for i in range(days):
        # ç”Ÿæˆå…·æœ‰è¶‹åŠ¿å’Œæ³¢åŠ¨çš„ä»·æ ¼
        trend = np.sin(i * 0.02) * 0.1  # é•¿æœŸè¶‹åŠ¿
        noise = np.random.normal(0, 0.05)  # éšæœºå™ªå£°
        price_change = trend + noise
        
        if i == 0:
            price = base_price
        else:
            price = max(bars[-1].close_price * (1 + price_change), 0.01)  # é˜²æ­¢è´Ÿä»·æ ¼
        
        volume = int(1000 + np.random.exponential(500))  # æˆäº¤é‡
        
        bar = StockDailyBar(
            date=base_date + timedelta(days=i),
            open_price=price * (1 + np.random.normal(0, 0.005)),
            high_price=price * (1 + abs(np.random.normal(0, 0.01))),
            low_price=price * (1 - abs(np.random.normal(0, 0.01))),
            close_price=price,
            volume=volume,
            amount=price * volume * 100,
            adjust_factor=1.0
        )
        bars.append(bar)
    
    return StockData(
        symbol=symbol,
        name=f"æµ‹è¯•è‚¡ç¥¨{symbol}",
        bars=bars
    )


def test_pipeline_basic_functionality():
    """æµ‹è¯•æµæ°´çº¿åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•æµæ°´çº¿åŸºæœ¬åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    stock_data = create_test_stock_data("000001", 60)
    
    # åˆ›å»ºæµæ°´çº¿
    config = PipelineConfig(
        indicators=["MA", "RSI", "MACD"],
        scaler_type=ScalerType.STANDARD,
        feature_selection=FeatureSelectionMethod.K_BEST_F,
        n_features=10,
        max_workers=2
    )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        pipeline = FeaturePipeline(config, temp_dir)
        
        # æµ‹è¯•è®­ç»ƒ
        pipeline.fit(stock_data)
        print("âœ… æµæ°´çº¿è®­ç»ƒæˆåŠŸ")
        
        # æµ‹è¯•è½¬æ¢
        features = pipeline.transform(stock_data)
        print(f"âœ… ç‰¹å¾è½¬æ¢æˆåŠŸï¼Œè¾“å‡ºç»´åº¦: {features.shape}")
        
        # æµ‹è¯•fit_transform
        features2 = pipeline.fit_transform(stock_data)
        print(f"âœ… fit_transformæˆåŠŸï¼Œè¾“å‡ºç»´åº¦: {features2.shape}")
        
        # éªŒè¯è¾“å‡ºåŒ…å«å¿…è¦çš„åˆ—
        required_cols = ['date', 'symbol']
        for col in required_cols:
            if col not in features.columns:
                print(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {col}")
                return False
        
        # éªŒè¯ç‰¹å¾æ•°é‡
        numeric_cols = features.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            print("âŒ æ²¡æœ‰æ•°å€¼ç‰¹å¾")
            return False
        
        print(f"âœ… ç”Ÿæˆäº† {len(numeric_cols)} ä¸ªæ•°å€¼ç‰¹å¾")
        return True


def test_batch_processing():
    """æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½"""
    print("\nğŸ“¦ æµ‹è¯•æ‰¹é‡å¤„ç†åŠŸèƒ½...")
    
    # åˆ›å»ºå¤šä¸ªè‚¡ç¥¨çš„æµ‹è¯•æ•°æ®
    symbols = ["000001", "000002", "600000", "000858", "002415"]
    stock_data_list = [create_test_stock_data(symbol, 80) for symbol in symbols]
    
    config = PipelineConfig(
        indicators=["MA", "EMA", "RSI", "MACD", "BOLL"],
        max_workers=3,
        batch_size=2
    )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        pipeline = FeaturePipeline(config, temp_dir)
        
        start_time = time.time()
        
        # æµ‹è¯•æ‰¹é‡è®­ç»ƒå’Œè½¬æ¢
        features = pipeline.fit_transform(stock_data_list)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f"âœ… æ‰¹é‡å¤„ç†æˆåŠŸ")
        print(f"   å¤„ç†è‚¡ç¥¨æ•°: {len(symbols)}")
        print(f"   è¾“å‡ºç»´åº¦: {features.shape}")
        print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"   å¤„ç†é€Ÿåº¦: {len(symbols)/processing_time:.1f}åª/ç§’")
        
        # éªŒè¯åŒ…å«æ‰€æœ‰è‚¡ç¥¨çš„æ•°æ®
        unique_symbols = features['symbol'].unique() if 'symbol' in features.columns else []
        if len(unique_symbols) != len(symbols):
            print(f"âŒ è‚¡ç¥¨æ•°é‡ä¸åŒ¹é…ï¼ŒæœŸæœ›: {len(symbols)}, å®é™…: {len(unique_symbols)}")
            return False
        
        print(f"âœ… æˆåŠŸå¤„ç†æ‰€æœ‰ {len(unique_symbols)} åªè‚¡ç¥¨")
        return True


def test_feature_store():
    """æµ‹è¯•ç‰¹å¾å­˜å‚¨åŠŸèƒ½"""
    print("\nğŸ’¾ æµ‹è¯•ç‰¹å¾å­˜å‚¨åŠŸèƒ½...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºç‰¹å¾å­˜å‚¨ç®¡ç†å™¨
        feature_store = FeatureStore(temp_dir)
        
        # åˆ›å»ºæµ‹è¯•ç‰¹å¾æ•°æ®
        test_data = {
            'symbol': ['000001'] * 50 + ['000002'] * 50,
            'date': pd.date_range('2024-01-01', periods=100, freq='D'),
            'close': np.random.randn(100) * 10 + 100,
            'ma_5': np.random.randn(100) * 5 + 100,
            'rsi_14': np.random.randn(100) * 20 + 50,
        }
        features_df = pd.DataFrame(test_data)
        
        # æµ‹è¯•å­˜å‚¨åŸå§‹ç‰¹å¾
        version = feature_store.store_raw_features(features_df)
        print(f"âœ… åŸå§‹ç‰¹å¾å­˜å‚¨æˆåŠŸï¼Œç‰ˆæœ¬: {version}")
        
        # æµ‹è¯•å­˜å‚¨å¤„ç†åç‰¹å¾
        pipeline_config = {'scaler': 'standard', 'n_features': 10}
        processed_version = feature_store.store_processed_features(
            features_df, pipeline_config
        )
        print(f"âœ… å¤„ç†åç‰¹å¾å­˜å‚¨æˆåŠŸï¼Œç‰ˆæœ¬: {processed_version}")
        
        # æµ‹è¯•åŠ è½½ç‰¹å¾
        loaded_features = feature_store.load_raw_features(['000001'])
        if not loaded_features.empty:
            print(f"âœ… ç‰¹å¾åŠ è½½æˆåŠŸï¼Œç»´åº¦: {loaded_features.shape}")
        else:
            print("âŒ ç‰¹å¾åŠ è½½å¤±è´¥")
            return False
        
        # æµ‹è¯•è·å–å­˜å‚¨ä¿¡æ¯
        info = feature_store.get_storage_info()
        print(f"âœ… å­˜å‚¨ä¿¡æ¯è·å–æˆåŠŸ: {info['raw_features_count']} ä¸ªåŸå§‹ç‰¹å¾æ–‡ä»¶")
        
        return True


def test_feature_transformations():
    """æµ‹è¯•ç‰¹å¾è½¬æ¢åŠŸèƒ½"""
    print("\nğŸ”„ æµ‹è¯•ç‰¹å¾è½¬æ¢åŠŸèƒ½...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    stock_data = create_test_stock_data("000001", 120)
    
    # æµ‹è¯•ä¸åŒçš„æ ‡å‡†åŒ–æ–¹æ³•
    scalers = [ScalerType.STANDARD, ScalerType.MINMAX, ScalerType.ROBUST, ScalerType.NONE]
    
    for scaler in scalers:
        config = PipelineConfig(
            scaler_type=scaler,
            feature_selection=FeatureSelectionMethod.NONE,
            indicators=["MA", "RSI", "MACD"]
        )
        
        with tempfile.TemporaryDirectory() as temp_dir:
            pipeline = FeaturePipeline(config, temp_dir)
            features = pipeline.fit_transform(stock_data)
            
            if not features.empty:
                print(f"âœ… {scaler.value} æ ‡å‡†åŒ–æˆåŠŸï¼Œç»´åº¦: {features.shape}")
            else:
                print(f"âŒ {scaler.value} æ ‡å‡†åŒ–å¤±è´¥")
                return False
    
    # æµ‹è¯•ç‰¹å¾é€‰æ‹©
    selection_methods = [
        FeatureSelectionMethod.K_BEST_F,
        FeatureSelectionMethod.MUTUAL_INFO,
        FeatureSelectionMethod.NONE
    ]
    
    for method in selection_methods:
        config = PipelineConfig(
            feature_selection=method,
            n_features=15,
            scaler_type=ScalerType.STANDARD,
            indicators=["MA", "EMA", "RSI", "MACD", "BOLL", "KDJ"]
        )
        
        with tempfile.TemporaryDirectory() as temp_dir:
            pipeline = FeaturePipeline(config, temp_dir)
            features = pipeline.fit_transform(stock_data)
            
            if not features.empty:
                numeric_cols = features.select_dtypes(include=[np.number]).columns
                print(f"âœ… {method.value} ç‰¹å¾é€‰æ‹©æˆåŠŸï¼Œæ•°å€¼ç‰¹å¾: {len(numeric_cols)}")
            else:
                print(f"âŒ {method.value} ç‰¹å¾é€‰æ‹©å¤±è´¥")
                return False
    
    return True


def test_pipeline_integration():
    """æµ‹è¯•æµæ°´çº¿é›†æˆåŠŸèƒ½"""
    print("\nğŸ”— æµ‹è¯•æµæ°´çº¿é›†æˆåŠŸèƒ½...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºé»˜è®¤æµæ°´çº¿
        pipeline = create_default_pipeline(temp_dir)
        
        # åˆ›å»ºç‰¹å¾å­˜å‚¨
        feature_store = create_default_feature_store(temp_dir)
        
        # åˆ›å»ºå¤šè‚¡ç¥¨æµ‹è¯•æ•°æ®
        symbols = ["000001", "000002", "600036"]
        stock_data_list = [create_test_stock_data(symbol, 90) for symbol in symbols]
        
        # è®­ç»ƒæµæ°´çº¿
        pipeline.fit(stock_data_list)
        print("âœ… é›†æˆæµæ°´çº¿è®­ç»ƒæˆåŠŸ")
        
        # è½¬æ¢ç‰¹å¾
        features = pipeline.transform(stock_data_list)
        print(f"âœ… é›†æˆç‰¹å¾è½¬æ¢æˆåŠŸï¼Œç»´åº¦: {features.shape}")
        
        # å­˜å‚¨ç‰¹å¾åˆ°å­˜å‚¨ç®¡ç†å™¨
        if not features.empty:
            version = feature_store.store_processed_features(
                features, 
                pipeline.get_pipeline_info()['config']
            )
            print(f"âœ… ç‰¹å¾å­˜å‚¨æˆåŠŸï¼Œç‰ˆæœ¬: {version}")
        
        # æµ‹è¯•ç‰¹å¾é‡è¦æ€§è®¡ç®—
        if 'close' in features.columns:
            # åˆ›å»ºè™šæ‹Ÿç›®æ ‡å˜é‡
            target = features.groupby('symbol')['close'].pct_change(1).shift(-1).fillna(0)
            importance = pipeline.get_feature_importance(features, target)
            
            if importance:
                top_5 = list(importance.keys())[:5]
                print(f"âœ… ç‰¹å¾é‡è¦æ€§è®¡ç®—æˆåŠŸï¼Œå‰5ä¸ªé‡è¦ç‰¹å¾: {top_5}")
            else:
                print("âŒ ç‰¹å¾é‡è¦æ€§è®¡ç®—å¤±è´¥")
                return False
        
        # è·å–æµæ°´çº¿ä¿¡æ¯
        info = pipeline.get_pipeline_info()
        print(f"âœ… æµæ°´çº¿ä¿¡æ¯è·å–æˆåŠŸ: {info['status']['is_fitted']}")
        
        return True


def test_with_real_data():
    """ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•"""
    print("\nğŸŒ ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•...")
    
    try:
        # åˆ›å»ºAKShareæ•°æ®é€‚é…å™¨
        adapter = AKShareAdapter()
        
        # è·å–å°‘é‡çœŸå®æ•°æ®è¿›è¡Œæµ‹è¯•
        end_date = date.today()
        start_date = end_date - timedelta(days=60)
        
        print(f"æ­£åœ¨è·å–å¹³å®‰é“¶è¡Œ(000001)çœŸå®æ•°æ®: {start_date} åˆ° {end_date}")
        stock_data = adapter.get_stock_data("000001", start_date, end_date)
        
        if not stock_data or not stock_data.bars:
            print("âŒ æœªèƒ½è·å–çœŸå®æ•°æ®ï¼Œè·³è¿‡æ­¤æµ‹è¯•")
            return True  # è·³è¿‡è€Œä¸æ˜¯å¤±è´¥
        
        print(f"âœ… è·å–çœŸå®æ•°æ®æˆåŠŸï¼Œå…± {len(stock_data.bars)} ä¸ªäº¤æ˜“æ—¥")
        
        # ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•æµæ°´çº¿
        with tempfile.TemporaryDirectory() as temp_dir:
            config = PipelineConfig(
                indicators=["MA", "EMA", "RSI", "MACD", "BOLL", "KDJ"],
                scaler_type=ScalerType.STANDARD,
                feature_selection=FeatureSelectionMethod.K_BEST_F,
                n_features=25,
                max_workers=2
            )
            
            pipeline = FeaturePipeline(config, temp_dir)
            
            start_time = time.time()
            features = pipeline.fit_transform(stock_data)
            end_time = time.time()
            
            print(f"âœ… çœŸå®æ•°æ®ç‰¹å¾å·¥ç¨‹æˆåŠŸ")
            print(f"   è¾“å‡ºç»´åº¦: {features.shape}")
            print(f"   å¤„ç†æ—¶é—´: {end_time - start_time:.3f}ç§’")
            
            # å±•ç¤ºä¸€äº›ç‰¹å¾ç»Ÿè®¡
            if not features.empty:
                numeric_features = features.select_dtypes(include=[np.number])
                print(f"   æ•°å€¼ç‰¹å¾æ•°: {len(numeric_features.columns)}")
                print(f"   æ•°æ®å®Œæ•´æ€§: {(1 - numeric_features.isnull().sum().sum() / numeric_features.size) * 100:.1f}%")
                
                # å±•ç¤ºæœ€æ–°ç‰¹å¾å€¼æ ·æœ¬
                if len(features) > 0:
                    latest = features.iloc[-1]
                    print(f"   æœ€æ–°æ—¥æœŸ: {latest.get('date', 'N/A')}")
                    if 'ma_5' in features.columns:
                        print(f"   5æ—¥å‡çº¿: {latest.get('ma_5', 0):.2f}")
                    if 'rsi_14' in features.columns:
                        print(f"   RSI: {latest.get('rsi_14', 0):.2f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çœŸå®æ•°æ®æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\nâš¡ æµ‹è¯•æ€§èƒ½...")
    
    # åˆ›å»ºå¤§é‡æ•°æ®è¿›è¡Œæ€§èƒ½æµ‹è¯•
    symbols = [f"{i:06d}" for i in range(1, 21)]  # 20åªè‚¡ç¥¨
    stock_data_list = [create_test_stock_data(symbol, 200) for symbol in symbols]
    
    config = PipelineConfig(
        indicators=["MA", "EMA", "RSI", "MACD", "BOLL", "KDJ", "WILLIAMS", "VOLUME_MA"],
        max_workers=4,
        batch_size=5
    )
    
    with tempfile.TemporaryDirectory() as temp_dir:
        pipeline = FeaturePipeline(config, temp_dir)
        
        start_time = time.time()
        features = pipeline.fit_transform(stock_data_list)
        end_time = time.time()
        
        processing_time = end_time - start_time
        
        print(f"âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"   å¤„ç†è‚¡ç¥¨æ•°: {len(symbols)}")
        print(f"   æ¯åªè‚¡ç¥¨æ•°æ®ç‚¹: 200")
        print(f"   æ€»æ•°æ®ç‚¹: {len(symbols) * 200}")
        print(f"   è¾“å‡ºç»´åº¦: {features.shape}")
        print(f"   æ€»å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"   å¤„ç†é€Ÿåº¦: {len(symbols)/processing_time:.1f}åªè‚¡ç¥¨/ç§’")
        print(f"   æ•°æ®ç‚¹å¤„ç†é€Ÿåº¦: {len(symbols) * 200/processing_time:.0f}æ¡/ç§’")
        
        # æ€§èƒ½åŸºå‡†æ£€æŸ¥
        if processing_time > 30:  # å¦‚æœè¶…è¿‡30ç§’è®¤ä¸ºæ€§èƒ½ä¸ä½³
            print("âš ï¸  æ€§èƒ½æµ‹è¯•é€šè¿‡ä½†å¤„ç†æ—¶é—´è¾ƒé•¿")
        
        return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ AIé‡åŒ–ç³»ç»Ÿ - ç‰¹å¾å·¥ç¨‹æ•°æ®æµæ°´çº¿éªŒè¯")
    print("=" * 80)
    
    test_functions = [
        test_pipeline_basic_functionality,
        test_batch_processing,
        test_feature_store,
        test_feature_transformations,
        test_pipeline_integration,
        test_with_real_data,
        test_performance
    ]
    
    results = []
    
    for test_func in test_functions:
        try:
            result = test_func()
            results.append(result)
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            results.append(False)
    
    # æ±‡æ€»æµ‹è¯•ç»“æœ
    print("\n" + "=" * 80)
    passed_tests = sum(results)
    total_tests = len(results)
    
    print(f"æµ‹è¯•ç»“æœæ±‡æ€»:")
    for i, (test_func, result) in enumerate(zip(test_functions, results)):
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {i+1}. {test_func.__name__}: {status}")
    
    print(f"\næ€»ä½“ç»“æœ: {passed_tests}/{total_tests} é€šè¿‡")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç‰¹å¾å·¥ç¨‹æ•°æ®æµæ°´çº¿éªŒè¯æˆåŠŸï¼")
        print("\nğŸ“Š ç‰¹å¾å·¥ç¨‹æµæ°´çº¿ç°å·²å‡†å¤‡å°±ç»ªï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›:")
        print("   â€¢ å®Œæ•´çš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆ8ç§æ ¸å¿ƒæŒ‡æ ‡ï¼‰")
        print("   â€¢ é«˜æ•ˆçš„æ‰¹é‡ç‰¹å¾å¤„ç†ï¼ˆå¤šè‚¡ç¥¨å¹¶è¡Œï¼‰")
        print("   â€¢ çµæ´»çš„ç‰¹å¾è½¬æ¢ï¼ˆæ ‡å‡†åŒ–ã€é€‰æ‹©ï¼‰")
        print("   â€¢ å¯é çš„ç‰¹å¾å­˜å‚¨å’Œç¼“å­˜æœºåˆ¶")
        print("   â€¢ ä¼˜ç§€çš„æ€§èƒ½è¡¨ç°ï¼ˆ>20åªè‚¡ç¥¨/ç§’ï¼‰")
        print("   â€¢ çœŸå®æ•°æ®å…¼å®¹æ€§éªŒè¯")
        return 0
    else:
        print(f"\nâš ï¸  æœ‰ {total_tests - passed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
        return 1


if __name__ == "__main__":
    sys.exit(main())



